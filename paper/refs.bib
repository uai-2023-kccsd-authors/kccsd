@article{Hyndman1996,
  author = {Rob J. Hyndman},
  title = {Computing and Graphing Highest Density Regions},
  journal = {The American Statistician},
  volume = {50},
  number = {2},
  pages = {120-126},
  year  = {1996},
  publisher = {Taylor & Francis},
  doi = {10.1080/00031305.1996.10474359},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1996.10474359},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00031305.1996.10474359}
}
@article{murray2017diff,
  title={Differential geometry and statistics},
  author={Murray, Michael K., and John W. Rice.},
  journal={Routledge},
  year={2017}
}
@article{widmann2022calibration,
  title={Calibration tests beyond classification},
  author={Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
  journal={arXiv preprint arXiv:2210.13355},
  year={2022}
}
@inproceedings{vaicenavicius2019evaluating,
  title={Evaluating model calibration in classification},
  author={Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3459--3467},
  year={2019},
  organization={PMLR}
}
@article{dalmasso2021likelihood,
  title={Likelihood-free frequentist inference: Bridging classical statistics and machine learning in simulation and uncertainty quantification},
  author={Dalmasso, Niccolo and Zhao, David and Izbicki, Rafael and Lee, Ann B},
  journal={arXiv preprint arXiv:2107.03920},
  year={2021}
}
@article{delaunoy2022towards,
  title={Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation},
  author={Delaunoy, Arnaud and Hermans, Joeri and Rozet, Fran{\c{c}}ois and Wehenkel, Antoine and Louppe, Gilles},
  journal={arXiv e-prints},
  pages={arXiv--2208},
  year={2022}
}
@article{talts2018validating,
  title={Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
  author={Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
  journal={arXiv preprint arXiv:1804.06788},
  year={2018}
}
@article{masserano2022simulation,
  title={Simulation-Based Inference with WALDO: Perfectly Calibrated Confidence Regions Using Any Prediction or Posterior Estimation Algorithm},
  author={Masserano, Luca and Dorigo, Tommaso and Izbicki, Rafael and Kuusela, Mikael and Lee, Ann B},
  journal={arXiv preprint arXiv:2205.15680},
  year={2022}
}
@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
  publisher={JMLR. org}
}
@article{widmann2019calibration,
  title={Calibration tests in multi-class classification: A unifying framework},
  author={Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{zhao2021diagnostics,
  title={Diagnostics for conditional density models and bayesian inference algorithms},
  author={Zhao, David and Dalmasso, Niccol{\`o} and Izbicki, Rafael and Lee, Ann B},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1830--1840},
  year={2021},
  organization={PMLR}
}
@inproceedings{jitkrittum2020testing,
  title={Testing goodness of fit of conditional density models with kernels},
  author={Jitkrittum, Wittawat and Kanagawa, Heishiro and Sch{\"o}lkopf, Bernhard},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={221--230},
  year={2020},
  organization={PMLR}
}
@article{DBLP:journals/jmlr/CuturiFV05,
  author    = {Marco Cuturi and
               Kenji Fukumizu and
               Jean{-}Philippe Vert},
  title     = {Semigroup Kernels on Measures},
  journal   = {J. Mach. Learn. Res.},
  volume    = {6},
  pages     = {1169--1198},
  year      = {2005},
  url       = {http://jmlr.org/papers/v6/cuturi05a.html},
  timestamp = {Wed, 10 Jul 2019 15:28:23 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/CuturiFV05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book {berg1984,
    AUTHOR = {Berg, Christian and Christensen, Jens Peter Reus and Ressel,
              Paul},
     TITLE = {Harmonic analysis on semigroups},
    SERIES = {Graduate Texts in Mathematics},
    VOLUME = {100},
      NOTE = {Theory of positive definite and related functions},
 PUBLISHER = {Springer-Verlag, New York},
      YEAR = {1984},
     PAGES = {x+289},
      ISBN = {0-387-90925-7},
   MRCLASS = {43-02 (43A35 60B15 60E15)},
  MRNUMBER = {747302},
MRREVIEWER = {Walter Schempp},
       DOI = {10.1007/978-1-4612-1128-0},
       URL = {https://doi.org/10.1007/978-1-4612-1128-0},
}
@inproceedings{DBLP:conf/aistats/0001GPS15,
  author    = {Zolt{\'{a}}n Szab{\'{o}} and
               Arthur Gretton and
               Barnab{\'{a}}s P{\'{o}}czos and
               Bharath K. Sriperumbudur},
  editor    = {Guy Lebanon and
               S. V. N. Vishwanathan},
  title     = {Two-stage sampled learning theory on distributions},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial
               Intelligence and Statistics, {AISTATS} 2015, San Diego, California,
               USA, May 9-12, 2015},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {38},
  publisher = {JMLR.org},
  year      = {2015},
  url       = {http://proceedings.mlr.press/v38/szabo15.html},
  timestamp = {Wed, 29 May 2019 08:41:44 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/0001GPS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/jmlr/SzaboSPG16,
  author    = {Zolt{\'{a}}n Szab{\'{o}} and
               Bharath K. Sriperumbudur and
               Barnab{\'{a}}s P{\'{o}}czos and
               Arthur Gretton},
  title     = {Learning Theory for Distribution Regression},
  journal   = {J. Mach. Learn. Res.},
  volume    = {17},
  pages     = {152:1--152:40},
  year      = {2016},
  url       = {http://jmlr.org/papers/v17/14-510.html},
  timestamp = {Wed, 10 Jul 2019 15:28:03 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/SzaboSPG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/icml/MeunierPC22,
  author    = {Dimitri Meunier and
               Massimiliano Pontil and
               Carlo Ciliberto},
  editor    = {Kamalika Chaudhuri and
               Stefanie Jegelka and
               Le Song and
               Csaba Szepesv{\'{a}}ri and
               Gang Niu and
               Sivan Sabato},
  title     = {Distribution Regression with Sliced Wasserstein Kernels},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
               2022, Baltimore, Maryland, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {15501--15523},
  publisher = {{PMLR}},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/meunier22b.html},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/MeunierPC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/jmiv/BonneelRPP15,
  author    = {Nicolas Bonneel and
               Julien Rabin and
               Gabriel Peyr{\'{e}} and
               Hanspeter Pfister},
  title     = {Sliced and Radon Wasserstein Barycenters of Measures},
  journal   = {J. Math. Imaging Vis.},
  volume    = {51},
  number    = {1},
  pages     = {22--45},
  year      = {2015},
  url       = {https://doi.org/10.1007/s10851-014-0506-3},
  doi       = {10.1007/s10851-014-0506-3},
  timestamp = {Sun, 02 Oct 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/journals/jmiv/BonneelRPP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/jmlr/MartinsSXAF09,
  author    = {Andr{\'{e}} F. T. Martins and
               Noah A. Smith and
               Eric P. Xing and
               Pedro M. Q. Aguiar and
               M{\'{a}}rio A. T. Figueiredo},
  title     = {Nonextensive Information Theoretic Kernels on Measures},
  journal   = {J. Mach. Learn. Res.},
  volume    = {10},
  pages     = {935--975},
  year      = {2009},
  url       = {https://dl.acm.org/doi/10.5555/1577069.1577104},
  doi       = {10.5555/1577069.1577104},
  timestamp = {Thu, 02 Jun 2022 13:58:57 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/MartinsSXAF09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/jmlr/LaffertyL05,
  author    = {John D. Lafferty and
               Guy Lebanon},
  title     = {Diffusion Kernels on Statistical Manifolds},
  journal   = {J. Mach. Learn. Res.},
  volume    = {6},
  pages     = {129--163},
  year      = {2005},
  url       = {http://jmlr.org/papers/v6/lafferty05a.html},
  timestamp = {Wed, 10 Jul 2019 15:28:09 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/LaffertyL05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hein2005hilbertian,
  title={Hilbertian metrics and positive definite kernels on probability measures},
  author={Hein, Matthias and Bousquet, Olivier},
  booktitle={International Workshop on Artificial Intelligence and Statistics},
  pages={136--143},
  year={2005},
  organization={PMLR}
}

@InProceedings{yang2018discrete,
  title = 	 {Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy},
  author =       {Yang, Jiasen and Liu, Qiang and Rao, Vinayak and Neville, Jennifer},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5561--5570},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yang18c/yang18c.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yang18c.html},
  abstract = 	 {Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.}
}

@misc{shi2022discrete,
  doi = {10.48550/ARXIV.2202.09497},
  url = {https://arxiv.org/abs/2202.09497},
  author = {Shi, Jiaxin and Zhou, Yuhao and Hwang, Jessica and Titsias, Michalis K. and Mackey, Lester},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Gradient Estimation with Discrete Stein Operators},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@book{johnson2004information,
  title={Information theory and the central limit theorem},
  author={Johnson, Oliver},
  year={2004},
  publisher={World Scientific}
}

@article{sriperumbudur2017density,
  title={Density estimation in infinite dimensional exponential families},
  author={Sriperumbudur, Bharath and Fukumizu, Kenji and Gretton, Arthur and Hyv{\"a}rinen, Aapo and Kumar, Revant},
  journal={Journal of Machine Learning Research},
  volume={18},
  year={2017}
}
@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching.},
  author={Hyv{\"a}rinen, Aapo and Dayan, Peter},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={4},
  year={2005}
}
@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{anastasiou2022stein,
  title={Stein’s method meets computational statistics: a review of some recent developments},
  author={Anastasiou, Andreas and Barp, Alessandro and Briol, Fran{\c{c}}ois-Xavier and Ebner, Bruno and Gaunt, Robert E and Ghaderinezhad, Fatemeh and Gorham, Jackson and Gretton, Arthur and Ley, Christophe and Liu, Qiang and others},
  journal={Statistical Science},
  volume={1},
  number={1},
  pages={1--20},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}
@article{gorham2019measuring,
  title={Measuring sample quality with diffusions},
  author={Gorham, Jackson and Duncan, Andrew B and Vollmer, Sebastian J and Mackey, Lester},
  journal={The Annals of Applied Probability},
  volume={29},
  number={5},
  pages={2884--2928},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}
@article{gorham2015measuring,
  title={Measuring sample quality with Stein's method},
  author={Gorham, Jackson and Mackey, Lester},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}
@inproceedings{gorham2017measuring,
  title={Measuring sample quality with kernels},
  author={Gorham, Jackson and Mackey, Lester},
  booktitle={International Conference on Machine Learning},
  pages={1292--1301},
  year={2017},
  organization={PMLR}
}
@misc{liu2016short,
  title={A short introduction to kernelized stein discrepancy},
  author={Liu, Qiang},
  year={2016}
}

@book{ambrosio2005gradient,
  title={Gradient flows: in metric spaces and in the space of probability measures},
  author={Ambrosio, Luigi and Gigli, Nicola and Savar{\'e}, Giuseppe},
  year={2005},
  publisher={Springer Science \& Business Media}
}
@article{carrillo2003kinetic,
  title={Kinetic equilibration rates for granular media and related equations: entropy dissipation and mass transportation estimates},
  author={Carrillo, Jos{\'e} A and McCann, Robert J and Villani, C{\'e}dric},
  journal={Revista Matematica Iberoamericana},
  volume={19},
  number={3},
  pages={971--1018},
  year={2003}
}
@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in Applied Probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}
@inproceedings{hermans2020likelihood,
  title={Likelihood-free mcmc with amortized approximate ratio estimators},
  author={Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
  booktitle={International Conference on Machine Learning},
  pages={4239--4248},
  year={2020},
  organization={PMLR}
}
@inproceedings{papamakarios2019sequential,
  title={Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows},
  author={Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={837--848},
  year={2019},
  organization={PMLR}
}

@article{brocker2008some,
  title={Some remarks on the reliability of categorical probability forecasts},
  author={Br{\"o}cker, Jochen},
  journal={Monthly weather review},
  volume={136},
  number={11},
  pages={4488--4502},
  year={2008}
}

@inproceedings{zadrozny2001obtaining,
  title={Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers},
  author={Zadrozny, Bianca and Elkan, Charles},
  booktitle={Icml},
  volume={1},
  pages={609--616},
  year={2001},
  organization={Citeseer}
}

@article{carmeli2010vector,
  title={Vector valued reproducing kernel Hilbert spaces and universality},
  author={Carmeli, Claudio and De Vito, Ernesto and Toigo, Alessandro and Umanit{\'a}, Veronica},
  journal={Analysis and Applications},
  volume={8},
  number={01},
  pages={19--61},
  year={2010},
  publisher={World Scientific}
}
@article{schrab2022efficient,
  title={Efficient Aggregated Kernel Tests using Incomplete $ U $-statistics},
  author={Schrab, Antonin and Kim, Ilmun and Guedj, Benjamin and Gretton, Arthur},
  journal={arXiv preprint arXiv:2206.09194},
  year={2022}
}

@article{zaremba2013b,
  title={B-test: A non-parametric, low variance kernel two-sample test},
  author={Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{jitkrittum2017linear,
  title={A linear-time kernel goodness-of-fit test},
  author={Jitkrittum, Wittawat and Xu, Wenkai and Szab{\'o}, Zolt{\'a}n and Fukumizu, Kenji and Gretton, Arthur},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@InProceedings{song2019distribution,
  title = {Distribution calibration for regression},
  author = {Song, H. and Diethe, T. and Kull, M. and Flach, P.},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages = {5897--5906},
  year = {2019},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  month = {6},
  publisher = {PMLR},
}

@InProceedings{kumar18train,
  author =       {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  title =        {Trainable Calibration Measures for Neural Networks
                  from Kernel Mean Embeddings},
  booktitle =    {Proceedings of the 35th International Conference on
                  Machine Learning},
  year =         2018,
  volume =       80,
  pages =        {2805--2814},
  abstract =     {Modern neural networks have recently been found to
                  be poorly calibrated, primarily in the direction of
                  over-confidence. Methods like entropy penalty and
                  temperature smoothing improve calibration by
                  clamping confidence, but in doing so compromise the
                  many legitimately confident predictions. We propose
                  a more principled fix that minimizes an explicit
                  calibration error during training. We present MMCE,
                  a RKHS kernel based measure of calibration that is
                  efficiently trainable alongside the negative
                  likelihood loss without careful hyper-parameter
                  tuning. Theoretically too, MMCE is a sound measure
                  of calibration that is minimized at perfect
                  calibration, and whose finite sample estimates are
                  consistent and enjoy fast convergence
                  rates. Extensive experiments on several network
                  architectures demonstrate that MMCE is a fast,
                  stable, and accurate method to minimize calibration
                  error while maximally preserving the number of high
                  confidence predictions.},
  pdf =
                  {http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf},
  series =       {Proceedings of Machine Learning Research},
}

@article{DeGroot1983,
  year = {1983},
  month = mar,
  publisher = {{JSTOR}},
  volume = {32},
  number = {1/2},
  pages = {12},
  author = {DeGroot, M. H. and Fienberg, S. E.},
  title = {The Comparison and Evaluation of Forecasters},
  journal = {The Statistician}
}

@article{Murphy1977,
  year = {1977},
  publisher = {{JSTOR}},
  volume = {26},
  number = {1},
  pages = {41},
  author = {A. H. Murphy and R. L. Winkler},
  title = {Reliability of Subjective Probability Forecasts of Precipitation and Temperature},
  journal = {Applied Statistics}
}

@article{Broecker2007,
  year = {2007},
  month = jun,
  publisher = {American Meteorological Society},
  volume = {22},
  number = {3},
  pages = {651--661},
  author = {J. Br\"{o}cker and L. A. Smith},
  title = {Increasing the Reliability of Reliability Diagrams},
  journal = {Weather and Forecasting}
}

@article{Broecker2009,
  year = {2009},
  month = jul,
  publisher = {Wiley},
  volume = {135},
  number = {643},
  pages = {1512--1519},
  author = {J. Br\"{o}cker},
  title = {Reliability, sufficiency, and the decomposition of proper scores},
  journal = {Quarterly Journal of the Royal Meteorological Society}
}

@InProceedings{Guo2017,
  title = {On Calibration of Modern Neural Networks},
  author = {C. Guo and G. Pleiss and Y. Sun and K. Q. Weinberger},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {1321--1330},
  year = {2017},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  month = {8},
  publisher = {PMLR},
}

@InProceedings{Kull2017,
  title = {Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers},
  author = {Kull, M. and Silva Filho, T. and Flach, P.},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = {623--631},
  year = {2017},
  volume = {54},
  series = {Proceedings of Machine Learning Research},
  month = {4},
  publisher = {PMLR},
}

@incollection{Kull2019,
  title = {Beyond temperature scaling: {Obtaining} well-calibrated multi-class probabilities with {Dirichlet} calibration},
  author = {Kull, M. and Perello Nieto, M. and K\"{a}ngsepp, M. and Silva Filho, T. and Song, H. and Flach, P.},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages = {12316--12326},
  year = {2019},
}

@inbook{Platt2000,
  author = {J. Platt},
  editor = {A. J. {Smola} and P. {Bartlett} and B. {Schölkopf} and D. {Schuurmans}},
  booktitle = {Advances in Large-Margin Classifiers},
  title = {Probabilities for SV Machines},
  year = {2000},
  pages = {61--73},
  publisher = {MIT Press},
}

@article{Fasiolo2020,
  year = {2020},
  month = mar,
  publisher = {Informa {UK} Limited},
  pages = {1--11},
  author = {M. Fasiolo and S. N. Wood and M. Zaffran and R. Nedellec and Y. Goude},
  title = {Fast Calibrated Additive Quantile Regression},
  journal = {Journal of the American Statistical Association}
}

@article{Rueda2006,
  year = {2006},
  month = dec,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {66},
  number = {3},
  pages = {355--371},
  author = {M. Rueda and S. Martinez-Puertas and H. Martinez-Puertas and A. Arcos},
  title = {Calibration methods for estimating quantiles},
  journal = {Metrika}
}

@article{Ho2005,
  year = {2005},
  month = mar,
  publisher = {Oxford University Press ({OUP})},
  volume = {92},
  number = {1},
  pages = {234--241},
  author = {Y. H. S. Ho and S. M. S. Lee},
  title = {Calibrated interpolated confidence intervals for population quantiles},
  journal = {Biometrika}
}

@article{Taillardat2016,
  year = {2016},
  month = jun,
  publisher = {American Meteorological Society},
  volume = {144},
  number = {6},
  pages = {2375--2393},
  author = {M. Taillardat and O. Mestre and M. Zamo and P. Naveau},
  title = {Calibrated Ensemble Forecasts Using Quantile Regression Forests and Ensemble Model Output Statistics},
  journal = {Monthly Weather Review}
}

@incollection{Zadrozny2002,
  title = {Reducing multiclass to binary by coupling probability estimates},
  author = {B. Zadrozny},
  booktitle = {Advances in Neural Information Processing Systems 14},
  pages = {1041--1048},
  year = {2002},
  publisher = {MIT Press},
}

@InProceedings{Naeini2015,
  author = {M. P. Naeini and G. Cooper and M. Hauskrecht},
  title = {Obtaining Well Calibrated Probabilities Using {Bayesian} Binning},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year = {2015},
}

@book{Steinwart2008SVM,
  author = {Andreas Christmann, Ingo Steinwart},
  doi = {10.1007/978-0-387-77242-4},
  url = {https://doi.org/10.1007/978-0-387-77242-4},
  year = {2008},
  publisher = {Springer New York},
  title = {Support Vector Machines}
}
@article{otto2000generalization,
  title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
  author={Otto, Felix and Villani, C{\'e}dric},
  journal={Journal of Functional Analysis},
  volume={173},
  number={2},
  pages={361--400},
  year={2000},
  publisher={Elsevier}
}
@article{lyu2012interpretation,
  title={Interpretation and generalization of score matching},
  author={Lyu, Siwei},
  journal={arXiv preprint arXiv:1205.2629},
  year={2012}
}
@article{jordan1998variational,
  title={The variational formulation of the Fokker--Planck equation},
  author={Jordan, Richard and Kinderlehrer, David and Otto, Felix},
  journal={SIAM journal on mathematical analysis},
  volume={29},
  number={1},
  pages={1--17},
  year={1998},
  publisher={SIAM}
}
@book{rogers2000diffusions,
  title={Diffusions, markov processes, and martingales: Volume 1, foundations},
  author={Rogers, Leonard CG and Williams, David},
  volume={1},
  year={2000},
  publisher={Cambridge university press}
}
@article{nguyen2010estimating,
  title={Estimating divergence functionals and the likelihood ratio by convex risk minimization},
  author={Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={11},
  pages={5847--5861},
  year={2010},
  publisher={IEEE}
}
@article{wenliang2020blindness,
  title={Blindness of score-based methods to isolated components and mixing proportions},
  author={Wenliang, Li K and Kanagawa, Heishiro},
  journal={arXiv preprint arXiv:2008.10087},
  year={2020}
}
@article{zhang2022towards,
  title={Towards healing the blindness of score matching},
  author={Zhang, Mingtian and Key, Oscar and Hayes, Peter and Barber, David and Paige, Brooks and Briol, Fran{\c{c}}ois-Xavier},
  journal={arXiv preprint arXiv:2209.07396},
  year={2022}
}
@article{arbel2019maximum,
  title={Maximum mean discrepancy gradient flow},
  author={Arbel, Michael and Korba, Anna and Salim, Adil and Gretton, Arthur},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{zhou2008derivative,
  title={Derivative reproducing properties for kernel methods in learning theory},
  author={Zhou, Ding-Xuan},
  journal={Journal of computational and Applied Mathematics},
  volume={220},
  number={1-2},
  pages={456--463},
  year={2008},
  publisher={Elsevier}
}
@article{micchelli2005learning,
  title={On learning vector-valued functions},
  author={Micchelli, Charles A and Pontil, Massimiliano},
  journal={Neural computation},
  volume={17},
  number={1},
  pages={177--204},
  year={2005},
  publisher={MIT Press}
}

@misc{Lee2022TCal,
  doi = {10.48550/ARXIV.2203.01850},
  url = {https://arxiv.org/abs/2203.01850},
  author = {Lee, Donghwan and Huang, Xinmeng and Hassani, Hamed and Dobriban, Edgar},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {T-Cal: An optimal test for the calibration of predictive models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Chwialkowski16KGOF,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}
